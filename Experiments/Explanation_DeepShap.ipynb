{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4",
   "mount_file_id": "1jws_EOz0wCg92P06SFY5KMNzYXDGmk5p",
   "authorship_tag": "ABX9TyMsphWuvZYSbOsaIT40txxX"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Experiments and Evaluation of Shap\n",
    "- Python Notebook to generate DeepShap explanation maps across all correctly predicted samples to plot the temporal\n",
    "averages in Figure B4 and B5 (for plot protocol run 'Plot_temporalAverageMaps.py')\n",
    "- For execution via Colab:\n",
    "    - 1.) create colab account\n",
    "    - 2.) sync to colab drive (use colab app) and create shortcut in google drive (right click -> organise -> shortcut)\n",
    "    - 3.) adapt paths in 'Preliminaries'"
   ],
   "metadata": {
    "id": "e2fCHKLcEIjV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Install packages.\n",
    "!pip install scipy==1.10.1\n",
    "!pip install matplotlib==3.5.3\n",
    "!pip install keras\n",
    "!pip install shap"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "id": "tz-f9_vrIFwN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688903082993,
     "user_tz": -120,
     "elapsed": 20089,
     "user": {
      "displayName": "Philine Lou",
      "userId": "11745850249216413368"
     }
    },
    "outputId": "62232326-d0ab-4071-9b03-b0481aac5338"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.10/dist-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy==1.10.1) (1.22.4)\n",
      "Collecting matplotlib==3.5.3\n",
      "  Downloading matplotlib-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.9/11.9 MB\u001B[0m \u001B[31m47.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.5.3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.3) (1.16.0)\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.1\n",
      "    Uninstalling matplotlib-3.7.1:\n",
      "      Successfully uninstalled matplotlib-3.7.1\n",
      "Successfully installed matplotlib-3.5.3\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "matplotlib",
         "mpl_toolkits"
        ]
       }
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
      "Collecting shap\n",
      "  Downloading shap-0.42.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (547 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m547.1/547.1 kB\u001B[0m \u001B[31m24.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.22.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.65.0)\n",
      "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.1)\n",
      "Collecting slicer==0.0.7 (from shap)\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.56.4)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->shap) (67.7.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2022.7.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
      "Installing collected packages: slicer, shap\n",
      "Successfully installed shap-0.42.0 slicer-0.0.7\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eTBrYdCHzK4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688903164740,
     "user_tz": -120,
     "elapsed": 81750,
     "user": {
      "displayName": "Philine Lou",
      "userId": "11745850249216413368"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5d611aec-04db-4d59-d807-f36f62da4a42"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "# Import python packages.\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import json\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount = True)\n",
    "\n",
    "# import tensorflow.compat.v1.keras.backend as K\n",
    "# import tensorflow as tf\n",
    "# tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "# Install a local package.\n",
    "!pip install -e /content/drive/MyDrive/Climate_X_Quantus/QuantusClimate/. --user"
   ],
   "metadata": {
    "id": "HEvAcWvW7rau"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip freeze"
   ],
   "metadata": {
    "id": "7hFKWMafaF-f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "# Import local package.\n",
    "import sys\n",
    "sys.path.insert(0,'/content/drive/My Drive/Climate_X_Quantus/')\n",
    "import QuantusClimate as quantus"
   ],
   "metadata": {
    "id": "YK52JG5goMhF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {
    "id": "tAX_zLBa_RSy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "def map2layer(x, layer):\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n"
   ],
   "metadata": {
    "id": "ikE4Z5gwpQOB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def invert_year_output(ypred, startYear, classChunk, yearsall):\n",
    "    inverted_years = convert_fuzzyDecade_toYear(ypred, startYear,\n",
    "                                                    classChunk, yearsall)\n",
    "\n",
    "    return inverted_years\n",
    "\n",
    "def convert_fuzzyDecade_toYear(label, startYear, classChunk, yearsall):\n",
    "\n",
    "\n",
    "    print('SELECT END YEAR - HARD CODED IN FUNCTION')\n",
    "    years = np.arange(startYear - classChunk * 2, yearsall.max() + classChunk * 2)\n",
    "    # years = np.arange(startYear - classChunk * 2, 2080 + classChunk * 2)\n",
    "    chunks = years[::int(classChunk)] + classChunk / 2\n",
    "\n",
    "    return np.sum(label * chunks, axis=1)"
   ],
   "metadata": {
    "id": "WlfGtkJsvV81"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "def explain_all(\n",
    "               model: keras.Sequential,\n",
    "               XXt: np.ndarray,\n",
    "               Yyt: np.ndarray,\n",
    "               method: Any,\n",
    "               **params\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate explanation for all inputs\n",
    "    \"\"\"\n",
    "\n",
    "    startYear = params.get('startYear', 1920)\n",
    "    annType = params['XAI'].get('annType', 'class')\n",
    "\n",
    "    ### Define prediction error\n",
    "    yearsUnique = np.unique(Yyt)\n",
    "    percCutoff = 90\n",
    "    withinYearInc = 2.\n",
    "    errTolerance = withinYearInc\n",
    "\n",
    "    err = Yyt[:, 0] - invert_year_output(model.predict(XXt),\n",
    "                                             startYear, params['classChunk'], params['yall'])\n",
    "\n",
    "    base = method[1][\"base\"]\n",
    "    if 'MLP'in method[1][\"net\"]:\n",
    "      base = base.reshape((len(base),method[1][\"lat\"]*method[1][\"lon\"]))\n",
    "      inputs = XXt.reshape((len(XXt),method[1][\"lat\"]*method[1][\"lon\"]))\n",
    "    else:\n",
    "      base = base.reshape((len(base),method[1][\"lat\"],method[1][\"lon\"],1))\n",
    "      inputs = XXt.reshape((len(XXt),method[1][\"lat\"],method[1][\"lon\"],1))\n",
    "\n",
    "    exp = shap.DeepExplainer(model, base)\n",
    "\n",
    "    maps = np.empty(np.shape(XXt))\n",
    "\n",
    "    for i in np.arange(0, np.shape(XXt)[0]):\n",
    "\n",
    "      analyzer_output = exp.shap_values(inputs[i,np.newaxis,...],  ranked_outputs=1, check_additivity=False)\n",
    "      maps[i] = np.array(analyzer_output[0])\n",
    "\n",
    "    ### Compute the frequency of data at each point and the average relevance\n",
    "    ### normalized by the sum over the area and the frequency above the 90th\n",
    "    ### percentile of the map\n",
    "    yearsUnique = np.unique(Yyt)\n",
    "    if params['net'] == 'CNN':\n",
    "        dTM = maps.reshape((yearsUnique.shape[0],int(np.shape(maps)[0]/yearsUnique.shape[0]),np.shape(maps)[1]*np.shape(maps)[2]))\n",
    "        deepTaylorMaps = maps.reshape((np.shape(maps)[0],np.shape(maps)[1]*np.shape(maps)[2]))\n",
    "    else:\n",
    "        dTM = maps.reshape((yearsUnique.shape[0], int(np.shape(maps)[0] / yearsUnique.shape[0]), np.shape(maps)[1]))\n",
    "        deepTaylorMaps = maps\n",
    "\n",
    "    summaryX = np.nanmean(dTM, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "    return summaryX, dTM, maps\n"
   ],
   "metadata": {
    "id": "uIBs8gW3_V_y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries\n",
    "- Set raw_path to raw data path\n",
    "- Set save_path to DeepShap result path\n",
    "- Set net = 'MLP' for MLP-based evaluation of DeepShap\n",
    "- Set net = 'CNN' for CNN-based evaluation of DeepShap"
   ],
   "metadata": {
    "id": "kauWxgSSnyC2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Set experiment settings.\n",
    "import yaml\n",
    "\n",
    "exp_path = '/content/drive/MyDrive/Climate_X_Quantus/Experiment/'\n",
    "raw_path = '/content/drive/MyDrive/Climate_X_Quantus/Data/'\n",
    "save_path = '/content/drive/MyDrive/Climate_X_Quantus/Data/Training/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = yaml.load(open(exp_path + '/plot_config.yaml'), Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "# Experiment variables.\n",
    "net = 'CNN'\n",
    "params = config['params']\n",
    "params['net'] = net\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "zIB21em4n66u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Data"
   ],
   "metadata": {
    "id": "btIuU89W6aAz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the full data object.\n",
    "all = np.load(raw_path + 'Quantus/%s' + '/0/' + 'Postprocessed_data_ALL.npz', allow_pickle=True)\n",
    "background= all[\"Input\"].reshape(all[\"Input\"].shape[0], 1, len(all[\"wh\"][0]), len(all[\"wh\"][1]))\n",
    "\n",
    "# select a set of background examples to take an expectation over.\n",
    "background = background[np.random.choice(background.shape[0], 100, replace=False)]\n",
    "\n",
    "# Longitude and latitudes.\n",
    "lat = all['wh'][0]\n",
    "lon = all['wh'][1]\n",
    "\n",
    "del all"
   ],
   "metadata": {
    "id": "-MdztJuA6YiG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # Generate explanations.\n",
    "\n",
    "data = np.load(raw_path + 'Training/' 'Preprocessed_data_%s_CESM1_obs_20CRv3.npz' % net, allow_pickle=True)\n",
    "ins = data['XtrainS']\n",
    "inst = data['XtestS']\n",
    "Ytrain = data['Ytrain']\n",
    "Ytest = data['Ytest']\n",
    "\n",
    "# Reshape.\n",
    "x_batch = np.append(ins, inst, axis=0)\n",
    "y_batch = np.append(Ytrain, Ytest, axis=0)"
   ],
   "metadata": {
    "id": "mGCjrFLwoqc9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load model"
   ],
   "metadata": {
    "id": "eBz8AmkI_rIX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from keras.models import load_model\n",
    "import keras\n",
    "\n",
    "model = load_model(raw_path + '/Network/' + 'lens_%s_0_T2M_1.tf' % net, compile=False)\n",
    "\n",
    "# Run the model on a test sample, requiring a compilation.\n",
    "model.compile(optimizer=keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[keras.metrics.categorical_accuracy],)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Stq2LmFl6YyB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688903248567,
     "user_tz": -120,
     "elapsed": 3812,
     "user": {
      "displayName": "Philine Lou",
      "userId": "11745850249216413368"
     }
    },
    "outputId": "c46f4f43-9c44-4228-bef4-eac99ef51a3f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The `lr` argument is deprecated, use `learning_rate` instead.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create explanations SHAP"
   ],
   "metadata": {
    "id": "Rwn2s-NXBWpb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Reshape Data\n",
    "if 'MLP' in net:\n",
    "  backg = background.reshape((len(background),len(lat)*len(lon)))\n",
    "  x_b= x_batch.reshape((len(x_batch), len(lat)*len(lon)))\n",
    "\n",
    "else:\n",
    "  backg = background.reshape((len(background),len(lat),len(lon),1))\n",
    "  x_b = x_batch.reshape((len(x_batch), len(lat),len(lon),1))"
   ],
   "metadata": {
    "id": "cvxfbkNYKFiQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import shap\n",
    "\n",
    "params['yall'] = np.arange(1920, 2080 + 1, 1)\n",
    "\n",
    "# Explanation variables.\n",
    "xai_methods =[(\"DeepSHAP\", {\"base\": backg, \"lat\":len(lat),\"lon\":len(lon), \"net\":net}, \"DeepSHAP\")]\n",
    "\n",
    "summary, dTM, maps = explain_all(model, x_batch, y_batch, xai_methods[0], **params)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMHAZ39LAAZ-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1688903520494,
     "user_tz": -120,
     "elapsed": 271929,
     "user": {
      "displayName": "Philine Lou",
      "userId": "11745850249216413368"
     }
    },
    "outputId": "88465597-7fe2-47d4-e2d1-12459a54ddc1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "202/202 [==============================] - 7s 2ms/step\n",
      "SELECT END YEAR - HARD CODED IN FUNCTION\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "keras is no longer supported, please use tf.keras instead.\n",
      "Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "`tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Save files"
   ],
   "metadata": {
    "id": "0eMyanIM1trr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# format.\n",
    "xaimapsall_m = summary.reshape(1, 1,len(params['yall']), len(lat), len(lon))\n",
    "xaimapsAll_m = maps.reshape(1, 1, 40, len(params['yall']), len(lat), len(lon))\n",
    "\n",
    "# Save mean maps.\n",
    "np.savez(save_path + '%s/DeepShap_UAI_YearlyMaps_1_20ens_T2M_training_ALL_annual.npz' %net, values = xaimapsAll_m)\n",
    "\n",
    "# Save maps.\n",
    "np.savez(save_path + '%s/DeepShap_UAI_YearlyMaps_1_20ens_T2M_training_cleaned_annual.npz' %net, values= xaimapsall_m)\n",
    "\n"
   ],
   "metadata": {
    "id": "Og6pc6UO1Jh3"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}